# Introduction
This project utilizes PySpark to analyze home sales data from an AWS S3 bucket. The analysis focuses on various metrics, such as average home prices based on the number of bedrooms, bathrooms, square footage, and other factors. The project also demonstrates the use of caching and partitioning to optimize query performance.

# Installation
To run this project, you need to have PySpark and other dependencies installed. Follow the steps below to set up your environment:
Install PySpark:
pip install pyspark
pip install findspark

# Usage
Initialize PySpark:
import findspark
findspark.init()
Create a Spark Session:
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()

# Features
Read and process home sales data from a CSV file hosted on AWS S3.
Perform SQL queries to extract meaningful insights from the data.
Cache and uncache data to improve query performance.
Partition data by date built and save it in Parquet format for optimized storage and retrieval.

# Dependencies
PySpark
Findspark (optional)
Detailed documentation for PySpark can be found on the PySpark official documentation.

# Contributors
Krysta Sharp

# Acknowledgements
Portions of this code was generated by ChatGPT to correct errors to codes that couldn't be resolve. 
